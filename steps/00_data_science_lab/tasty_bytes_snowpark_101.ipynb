{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4b4bcbd",
   "metadata": {},
   "source": [
    "![Banner-3.png](<../../images/tasty_bytes/Banner-3.png>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bc09577",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Tasty Bytes is one of the largest food truck networks in the world with localized menu options spread across 30 major cities in 15 countries.<br>**Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n",
    "\n",
    "As Tasty Bytes Data Scientists, we have been asked to support this goal by helping our food truck drivers more intelligently pick where to park for shifts.<br>**We want to direct our trucks to locations that are expected to have the highest sales on a given shift. <br>This will maximize our daily revenue across our fleet of trucks.**\n",
    "\n",
    "To provide this insight, we will use historical shift sales at each location to build a model. This data has been made available to us in Snowflake.<br> \n",
    "\n",
    "Our model will provide the predicted sales at each location for the upcoming shift.\n",
    "\n",
    "![problem_overview.png](<../../images/tasty_bytes/problem_overview.png>)\n",
    "\n",
    "**This is an introduction to Snowpark for Snowflake.\n",
    "We will use Snowpark to:**\n",
    "\n",
    "- Explore the data\n",
    "- Perform feature engineering\n",
    "- Train a model\n",
    "- Deploy the model in Snowflake\n",
    "\n",
    "\n",
    "**Why Snowpark?**\n",
    "\n",
    "- No copies or movement of data\n",
    "- Maintain governance\n",
    "- Leverage Snowflake scalable compute\n",
    "- ...and more!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68d4b968",
   "metadata": {},
   "source": [
    "![snowpark_101.png](<../../images/tasty_bytes/snowpark_101.png>)\n",
    "Let's get to know Snowpark. We will see that Snowpark makes it easy for Python users to leverage the Snowflake platform. Bringing these users into the Snowflake platform will foster collaboration and  streamline architecture across all users and teams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95254c0d",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "Just like the Python packages we are importing, we will import the Snowpark modules that we need.<br>\n",
    "**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import json\n",
    "import sys\n",
    "import cachetools\n",
    "\n",
    "# Import Snowflake modules\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import Window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c7ff92",
   "metadata": {},
   "source": [
    "## Connect to Snowflake\n",
    "Our Snowflake role, **hol_role**, can access the data in the **analytics** schema of the **hol_db** database. We will use the **hol_wh** warehouse that has been created as a dedicated compute for data science workloads.\n",
    "\n",
    "We will use these parameters and our Snowflake account credentials to connect to Snowflake and create a Snowpark session.<br>\n",
    "**Value:** Secure and governed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.getOrCreate()\n",
    "\n",
    "session.sql(\"SELECT CURRENT_ROLE()\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8cba29d",
   "metadata": {},
   "source": [
    "## Snowpark DataFrame\n",
    "Let's create a Snowpark DataFrame containing our shift sales data from the **shift_sales_v** view in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.<br>\n",
    "**Value:** Familiar representation of data for Python users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85142cce-9541-4808-979f-37bca023c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df = session.table(\"hol_db.analytics.shift_sales_v\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "385ed1ac",
   "metadata": {},
   "source": [
    "## Preview the Data\n",
    "With our Snowpark DataFrame defined, letâ€™s use the .show() function to take a look at the first 10 rows. <br>\n",
    "**Value:** Instant access to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4580e338",
   "metadata": {},
   "source": [
    "## Select, Filter, Sort\n",
    "Notice the Null values for \"shift_sales\". Let's look at a single location.\n",
    "\n",
    "To do this, we will make another Snowpark DataFrame, location_df, from the above DataFrame and we will:\n",
    "1. Select columns\n",
    "2. Filter to a single location ID\n",
    "3. Sort by date\n",
    "\n",
    "**Value**: Efficient transformation pipelines using Python syntax and chained logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select\n",
    "location_df = snowpark_df.select(\"date\", \"shift\", \"shift_sales\", \"location_id\", \"city\")\n",
    "\n",
    "# Filter\n",
    "location_df = location_df.filter(F.col(\"location_id\") == 1135)\n",
    "\n",
    "# Sort\n",
    "location_df = location_df.order_by([\"date\", \"shift\"], ascending=[0, 0])\n",
    "\n",
    "# Display\n",
    "location_df.show(n=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06f63f51",
   "metadata": {},
   "source": [
    "We can see that shift sales are populated 8 days prior to the latest date in the data. The **missing values** represent future dates that do not have shift sales yet.\n",
    "\n",
    "\n",
    "## Snowpark works in two main ways:\n",
    "1. Snowpark code translated and executed as SQL on Snowflake\n",
    "2. Python functions deployed in a secure sandbox in Snowflake \n",
    "\n",
    "![snowpark_overview.png](<../../images/tasty_bytes/snowpark_overview.png>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41772a46",
   "metadata": {},
   "source": [
    "## Explain the Query\n",
    "Let's look at what was executed in Snowflake to create our location_df DataFrame.\n",
    "\n",
    "The translated SQL query can be seen in the Snowsight interface under *Activity* in the *Query History* or directly in our notebook by using the explain() function.\n",
    "![query_history.png](<../../images/tasty_bytes/query_history.png>)\n",
    "\n",
    "**Value:** Transparent execution and compute usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03027a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacf17bf",
   "metadata": {},
   "source": [
    "## Compare DataFrame Size\n",
    "Let's bring a sample of our Snowflake dataset to our Python environment in a pandas DataFrame using the to_pandas() function. We will compare how much memory is used for the pandas DataFrame compared to the Snowpark DataFrame. As we will see, no memory is used for the Snowpark DataFrame in our Python environment. All data in the Snowpark DataFrame remains on Snowflake.<br>\n",
    "\n",
    "**Value:** No copies or movement of data when working with Snowpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1207145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring 10,000 rows from Snowflake to pandas\n",
    "pandas_df = snowpark_df.limit(10000).to_pandas()\n",
    "\n",
    "# Get Snowpark DataFrame size\n",
    "snowpark_size = sys.getsizeof(snowpark_df) / (1024*1024)\n",
    "print(f\"Snowpark DataFrame Size (snowpark_df): {snowpark_size:.2f} MB\")\n",
    "\n",
    "# Get pandas DataFrame size\n",
    "pandas_size = sys.getsizeof(pandas_df) / (1024*1024)\n",
    "print(f\"Pandas DataFrame Size (pandas_df): {pandas_size:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b50307",
   "metadata": {},
   "source": [
    "![data_exploration.png](<../../images/tasty_bytes/data_exploration.png>)\n",
    "Here, we will use Snowpark to explore our data. A common pattern for exploration is to use Snowpark to manipulate our data and then bring an aggregate table to our Python environment for visualization. <br>\n",
    "\n",
    "**Value:** \n",
    "   - Native Snowflake performance and scale for aggregating large datasets. \n",
    "   - Easy transfer of aggregate data to the client-side environment for visualization.\n",
    "   \n",
    "<br>As we explore our data, we will highlight what is being done in Snowflake and what we are transferring to our client-side environment (Python notebook environment) for visualization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283cb5e6",
   "metadata": {},
   "source": [
    "## How many rows are in our data?\n",
    "This will give us an idea of how we might need to approach working with this data. Do we have enough data to build a meaningful model? What compute might be required? Will we need to sample the data?<br>\n",
    "**What's happening where?:** Rows counted in Snowflake. No data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b98fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7b90839",
   "metadata": {},
   "source": [
    "## Let's calculate some descriptive statistics.\n",
    "We use the Snowpark describe() function to calculate summary statistics and then bring the aggregate results into a pandas DataFrame to visualize in a formatted table.<br>\n",
    "**What's happening where?:** Summary statistics calculated in Snowflake. Transfer aggregate summary statistics for client-side visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0788a-b4e2-42af-bccf-c293558528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.describe().to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50c7ca73",
   "metadata": {},
   "source": [
    "## What are the numeric columns?\n",
    "We want to understand the data types in our data and how we might need to handle them in preparation for modeling. For numeric columns, this could include normalizing the data to the same scale or applying a transformation to change the distribution.<br>\n",
    "**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Snowflake numeric types\n",
    "numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in numeric_types]\n",
    "numeric_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0173f66",
   "metadata": {},
   "source": [
    "## What are the categorical columns?\n",
    "Our model requires all features to be numeric. We want to identify columns that we will need to transform to a numeric representation if we would like to use them as features in our model.<br>\n",
    "**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Snowflake categorical types\n",
    "categorical_types = [T.StringType]\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in categorical_types]\n",
    "categorical_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cd5f178",
   "metadata": {},
   "source": [
    "## What are the average shift sales (USD) by city?\n",
    "Here, we are trying to understand what a \"normal\" shift sale looks like. What is the span of averages across cities? Are there any outlier cities that should be removed from our training data? Is there anything unexpected in the order of cities sorted by their average shift sales?<br>\n",
    "**What's happening where?:** Average sales by city calculated in Snowflake. Transfer city averages for client-side visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and average shift sales\n",
    "analysis_df = snowpark_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "# Sort by average shift sales\n",
    "analysis_df = analysis_df.sort(\"avg_shift_sales\", ascending=True)\n",
    "\n",
    "# Pull to pandas and plot\n",
    "analysis_df.to_pandas().plot.barh(x=\"CITY\", y=\"AVG_SHIFT_SALES\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41d30aca",
   "metadata": {},
   "source": [
    "## Looking at Vancouver, how many locations are there?\n",
    "We are going to use Vancouver, Canada as our test city when predicting the best location for future shifts. Let's get to know locations and shift sales in that city. First, we will see how many location options there are in Vancouver for a food truck to park.<br>\n",
    "**What's happening where?:** Data filtered, averages calculated by location, and locations counted in Snowflake. No data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec48065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Vancouver\n",
    "analysis_df = snowpark_df.filter(F.col(\"city\") == \"Vancouver\")\n",
    "\n",
    "# Group by location and average shift sales\n",
    "analysis_df = analysis_df.group_by(\"location_id\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "# Get the location count\n",
    "print(\"Vancouver location count:\", analysis_df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e077500",
   "metadata": {},
   "source": [
    "## What is the distribution of average shift sales (USD) by location in Vancouver?\n",
    "The x-axis represents average shift sales (USD) and the y-axis represents the count of locations with that average. The highest bars indicate the most common average shift sales. We will keep this in mind when evaluating predictions.<br>\n",
    "**What's happening where?:** Data filtered and averages calculated by locations in Snowflake. Transfer location averages for client-side visualization.<br>\n",
    "**Value:** Aggregate where the data is stored using native Snowflake performance and scale. Easy transfer of aggregate data to client-side applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analysis_df.to_pandas().hist(column=\"AVG_SHIFT_SALES\", bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d66f82d",
   "metadata": {},
   "source": [
    "![feature_engineering.png](<../../images/tasty_bytes/feature_engineering.png>)\n",
    "We will create a feature engineering pipeline by chaining transformations to prepare our data for model training.<br>\n",
    "**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.<br>\n",
    "**All transformations for feature engineering in this notebook will be executed on Snowflake compute.**\n",
    "\n",
    "Notice what we haven't had to do? No tuning, maintenance, or operational overhead. We just need a role, warehouse, and access to the data.<br>\n",
    "**Value**: Near-zero maintenance. Focus on the work that brings value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec9f7acb",
   "metadata": {},
   "source": [
    "## Create a Rolling Average Feature\n",
    "We will use a Snowflake window function to get a **rolling shift average by location** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n",
    "\n",
    "**Step 1. Create a Window**\n",
    "\n",
    "Our window will partition the data by location and shift. It will order rows by date. It will include all rows prior to the current date of the observation it is aggregating for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2eec0-a63f-464e-9e2f-ca928c9ab0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_by_location_all_days = (\n",
    "    Window.partition_by(\"location_id\", \"shift\")\n",
    "    .order_by(\"date\")\n",
    "    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caf6e226",
   "metadata": {},
   "source": [
    "**Step 2. Aggregate across the Window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d4094-5cac-4c5b-bf2c-ac358d3b9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df = snowpark_df.with_column(\n",
    "    \"avg_location_shift_sales\", \n",
    "    F.avg(\"shift_sales\").over(window_by_location_all_days)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "301447d0",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "The rolling average feature we just created is missing if there are no prior shift sales at that location. We will replace those missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56cbd6-797e-4d88-ad4d-526dc39a1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df = snowpark_df.fillna(value=0, subset=[\"avg_location_shift_sales\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09f7b956",
   "metadata": {},
   "source": [
    "## Encode Categorical Columns\n",
    "Categorical columns need to be represented as numeric in our model. We will use binary encoding for the **shift** column by replacing:\n",
    "- \"AM\" with 1\n",
    "- \"PM\" with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1de475-5bdd-4628-a4cb-9cb79f994aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df = snowpark_df.with_column(\"shift\", F.iff(F.col(\"shift\") == \"AM\", 1, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2fb74af",
   "metadata": {},
   "source": [
    "## Filter to Historical Data\n",
    "Our data includes placeholders for future data with missing shift sales. The **future data** represents the next 7 days of shifts for all locations. The **historical data** has shift sales for all locations where a food truck parked during a shift. We will only use historical data when training our model and will filter out the dates where the **shift_sales** column is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4537b-5a1a-4a94-af4b-ab91bfa39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_snowpark_df = snowpark_df.filter(F.col(\"shift_sales\").is_not_null())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d026722c",
   "metadata": {},
   "source": [
    "## Drop Columns\n",
    "Drop ID columns that will not be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c05c5-fee7-466e-9707-47fab634f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_snowpark_df = historical_snowpark_df.drop(\"location_id\", \"city\", \"date\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "087cb29b",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing\n",
    "We will use 80% of the data for model training and 20% for testing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a85e60-f9cc-4be3-828b-8989b886573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"USE SCHEMA hol_db.analytics\").collect()\n",
    "train_snowpark_df, test_snowpark_df = historical_snowpark_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b97e058",
   "metadata": {},
   "source": [
    "## Save Tables in Snowflake\n",
    "We will save our training and test datasets to the **analytics schema** in our Snowflake account.<br>\n",
    "**Value:** Eliminate redundant data processing. These tables can be re-used to train more models beyond what we are training today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14603b-74ae-4932-b04a-47a7c7133305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data\n",
    "train_snowpark_df.write.mode(\"overwrite\").save_as_table(\"hol_db.analytics.shift_sales_train\")\n",
    "\n",
    "# Save test data\n",
    "test_snowpark_df.write.mode(\"overwrite\").save_as_table(\"hol_db.analytics.shift_sales_test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd8691b2",
   "metadata": {},
   "source": [
    "![model_training.png](<../../images/tasty_bytes/model_training.png>)\n",
    "We will now use our training data to train a linear regression model on Snowflake. \n",
    "\n",
    "Recall from above, the two main ways that Snowpark works: \n",
    "1. Snowpark code translated and executed as SQL on Snowflake\n",
    "2. Python functions deployed in a secure sandbox in Snowflake \n",
    "\n",
    "We will be leveraging the deployment of Python functions into Snowflake for training and model deployment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2880f7da",
   "metadata": {},
   "source": [
    "![end_to_end_ml.png](<../../images/tasty_bytes/end_to_end_ml.png>)\n",
    "Here, we see a typical data science workflow. We are finished **preparing** our data and now move on to **training in a Python stored procedure on Snowflake**. The model created from this stored procedure will be our tool for automating decisions around truck locations to maximize our revenue. We'll surface the predicted sales (model inference) on future data using a **Python user-defined function** to drive the decisions.\n",
    "\n",
    "**Snowflake Stored Procedures** work well for training because they can read data, hold an entire table in memory to find patterns, and write files (e.g. model files) back to the Snowflake database.\n",
    "\n",
    "**Snowflake User-Defined Functions** work well for inference because they return a single value for each row passed to the user-defined function. Because of this, they can easily be distributed to provide fast results.\n",
    "\n",
    "**Value**: Effortless, scalable, and secure processing **without data movement** across compute environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16e0c553",
   "metadata": {},
   "source": [
    "## Create a Stage\n",
    "We will use this stage to host user-defined functions, stored procedures, and model files on Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708586d6-9de4-400c-9259-17b8eee0ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stage\n",
    "session.sql(\"CREATE STAGE IF NOT EXISTS hol_db.analytics.model_stage\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a09a168",
   "metadata": {},
   "source": [
    "## Create a Stored Procedure for Model Training\n",
    "\n",
    "**Step 1. Create a Function for Training a Model**<br>\n",
    "This function trains a Scikit-learn linear regression model and saves the model to a stage. Linear regression finds a line that best fits the data points used in training. We then use that line as an estimation of where output values will be for future scenarios.\n",
    "\n",
    "Here, training will use historical shift sales and features in our data to predict future shift sales of locations where our food trucks can park.\n",
    "- **Inputs:** Training table name on Snowflake, feature column names, target column names, file name for the saved model\n",
    "- **Outputs:**  Feature weights of the trained model *(or feature contributions to the predicted value)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67050d8-772b-4c7f-bb8a-9d4f059093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(\n",
    "    session: Session,\n",
    "    training_table: str,\n",
    "    feature_cols: list,\n",
    "    target_col: str,\n",
    "    model_name: str,\n",
    ") -> T.Variant:\n",
    "\n",
    "    # Import packages\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from joblib import dump\n",
    "\n",
    "    # Get training data\n",
    "    df = session.table(training_table).to_pandas()\n",
    "\n",
    "    # Set inputs X and outputs y\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Train model\n",
    "    model = LinearRegression().fit(X, y)\n",
    "\n",
    "    # Get feature weights\n",
    "    feature_weights = pd.DataFrame({\"Feature\": model.feature_names_in_, \"Weight\": model.coef_}).to_dict()\n",
    "\n",
    "    # Save model\n",
    "    dump(model, \"/tmp/\" + model_name)\n",
    "    session.file.put(\n",
    "        \"/tmp/\" + model_name,\n",
    "        \"@HOL_DB.ANALYTICS.MODEL_STAGE\",\n",
    "        auto_compress=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # Return feature contributions\n",
    "    return feature_weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca234ccd",
   "metadata": {},
   "source": [
    "**Step 2. Register the Function on Snowflake**<br>\n",
    "To register the function on Snowflake as a stored procedure, specify what Python packages are required in the function. Here we specify:\n",
    "- Snowpark\n",
    "- Scikit-learn (for training our model)\n",
    "- Joblib (for creating a model file)\n",
    "\n",
    "Scikit-learn is a popular Python library for Machine Learning. We will be able to leverage its functionality in Snowflake in our deployed stored procedure.\n",
    "\n",
    "**Value:** Easy access to hundreds of Python packages with automated dependency management.<br>\n",
    "*Python packages available in Snowflake:* https://repo.anaconda.com/pkgs/snowflake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bba033-1075-4ccd-965b-52a47ee0fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_linreg_snowflake = session.sproc.register(\n",
    "    func=train_linreg,\n",
    "    name=\"sproc_train_linreg\",\n",
    "    is_permanent=True,\n",
    "    replace=True,\n",
    "    stage_location=\"@HOL_DB.ANALYTICS.MODEL_STAGE\",\n",
    "    packages=[\"snowflake-snowpark-python\", \"scikit-learn\", \"joblib\"]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea27bcec",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We will now train our model on Snowflake using our stored procedure. If more memory was required, a Snowpark Optimized Warehouse could be used. In this case, a standard Snowflake warehouse is sufficient.\n",
    "\n",
    "*Documentation for Snowpark Optimized Warehouses:* https://docs.snowflake.com/en/developer-guide/snowpark/python/python-snowpark-training-ml.html<br>\n",
    "**Value:** Always access the compute you need - no bottlenecks!\n",
    "\n",
    "**Call the Training Stored Procedure**<br><br>\n",
    "We will call our stored procedure and specify our training table location, feature and target columns, and our saved model name. This will perform model training in Snowflake.<br>\n",
    "**Value:** Secure - Python stored procedures and user-defined functions execute in highly secure sandboxed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d977edf-4a6c-45f5-bf6b-099a4ca1943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inputs\n",
    "training_table = \"hol_db.analytics.shift_sales_train\"\n",
    "model_name = \"linreg_location_sales_model.sav\"\n",
    "feature_cols = [\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"LATITUDE\",\n",
    "    \"LONGITUDE\",\n",
    "    \"CITY_POPULATION\",\n",
    "    \"AVG_LOCATION_SHIFT_SALES\",\n",
    "    \"SHIFT\",\n",
    "]\n",
    "target_col = \"SHIFT_SALES\"\n",
    "\n",
    "\n",
    "# Call the training stored procedure\n",
    "feature_contributions = train_linreg_snowflake(\n",
    "    session, training_table, feature_cols, target_col, model_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0c3e08",
   "metadata": {},
   "source": [
    "## Examine Feature Contributions\n",
    "Our stored procedure returns the feature contributions. Here we can see which features have the largest impact on shift sales predictions. The prediction provided by the linear regression model is a summation of the feature values multiplied by their respective weights (plus an additional bias term). We can see that people are hungrier in the afternoon!<br>\n",
    "**Value**: Training in a stored procedure isn't a black-box process. We can return the insight we need (in this case, the feature weights) or write to a table in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37173b81-0a0a-4408-956d-16e0fddeea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eval(feature_contributions)).sort_values(by=\"Weight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbfdd5c",
   "metadata": {},
   "source": [
    "## View the Saved Model\n",
    "Let's make sure our model is saved to the stage on Snowflake. We will list the files on the stage and see when they were last modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdade6c0-82d4-40d6-a647-331f9715d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(session.sql(\"LIST @HOL_DB.ANALYTICS.MODEL_STAGE\").collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f5792ad",
   "metadata": {},
   "source": [
    "![model_deployment.png](<../../images/tasty_bytes/model_deployment.png>)\n",
    "We need to make our model available in a central location where it can be used by other users and teams. \n",
    "- The application development team will need to integrate it with the truck driver app to surface recommendations. \n",
    "- The analyst team will use it to benchmark and measure the impact on sales. \n",
    "- As data scientists, we will want to monitor the performance and work to improve the model. \n",
    "\n",
    "What better location to deploy the model, than with the data being used by these teams?<br>\n",
    "**Value:** Democratize access to models and predictions.\n",
    "\n",
    "## Create a User-Defined Function for Model Inference\n",
    "**Step 1. Create a Function for Model Inference**<br>\n",
    "This function loads the saved model and predicts shift sales from input features\n",
    "- **Inputs:** Features\n",
    "- **Outputs:**  Predicted shift sales\n",
    "\n",
    "Because we specified the input as a pandas DataFrame and output as a pandas Series, the user-defined function will **execute in batches** instead of one row at a time. In many cases, this reduces the overall execution time for the query calling the user-defined function.\n",
    "\n",
    "We also use the Python library cachetools to cache the model that is returned from the load_model function. This ensures that the **model is loaded once** per user-defined function call, instead of for each row or batch in the data.\n",
    "\n",
    "**Value:** Fast and efficient predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ab313-edfd-4347-b6c6-99b3bbfc90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model from file and cache the result\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(filename):\n",
    "    \n",
    "    # Import packages\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    # Get the import directory where the model file is stored\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    \n",
    "    # Get the import directory where the model file is stored\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "# Function to predict shift sales\n",
    "def linreg_predict_location_sales(X: pd.DataFrame) -> pd.Series:\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(\"linreg_location_sales_model.sav\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Return rounded predictions\n",
    "    return predictions.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75eff187",
   "metadata": {},
   "source": [
    "**Step 2. Register the Function on Snowflake**<br>\n",
    "To register the function on Snowflake as a user-defined function, we specify the Python packages required. Here we specify:\n",
    "- Scikit-learn (for making predictions)\n",
    "- Joblib (for loading the model from file)\n",
    "- Cachetools (for caching the loaded model)\n",
    "\n",
    "Additionally, we specify which files to import with the user-defined function. Here we specify our trained model file as an import.<br>\n",
    "**Value:** Simple and painless model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.udf.register(\n",
    "    func=linreg_predict_location_sales,\n",
    "    name=\"udf_linreg_predict_location_sales\",\n",
    "    stage_location=\"@HOL_DB.ANALYTICS.MODEL_STAGE\",\n",
    "    input_types=[T.FloatType()] * len(feature_cols),\n",
    "    return_type=T.FloatType(),\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    imports=[\"@HOL_DB.ANALYTICS.MODEL_STAGE/linreg_location_sales_model.sav\"],\n",
    "    packages=[\"scikit-learn\", \"joblib\", \"cachetools\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ac1d15",
   "metadata": {},
   "source": [
    "## Predict!\n",
    "Now that our inference user-defined function is deployed, we can use it in Snowflake on our data to get shift sales predictions.\n",
    "\n",
    "**Scale Up Snowflake Compute**<br>\n",
    "We can perform distributed model inferencing across the nodes of our Snowflake warehouse by scaling up to a multi-node warehouse. By scaling up to a medium warehouse, the processing will distribute across all four warehouse nodes instead of using single-threaded execution on an x-small warehouse with 1 node. \n",
    "We haven't required a medium warehouse up to this point but can easily adjust our warehouse now to meet the compute requirements for this portion of our workflow.<br>\n",
    "\n",
    "**Value:** Cost-effective performance by adapting processing to machine learning needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = MEDIUM\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e157514",
   "metadata": {},
   "source": [
    "**Call the Inference Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train_snowpark_df.select(\n",
    "    \"shift_sales\",\n",
    "    F.call_udf(\"udf_linreg_predict_location_sales\", [F.col(c) for c in feature_cols]).alias(\"prediction\")\n",
    ")\n",
    "train_pred.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b7032e8",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "We will evaluate our model using Root-Mean-Square Error (RMSE) for both our training and test data. We will get predictions with our user-defined function and compare them to the actual historical shift sales.\n",
    "\n",
    "**Value:** Calculation of our evaluation metric and model inference happens on Snowflake compute. No data moves outside of Snowflake and governance is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train evalutation RMSE\n",
    "rmse_train = train_pred.select(F.sqrt(F.mean((F.col(\"shift_sales\") \n",
    "                                              - F.col(\"prediction\"))**2)))\n",
    "\n",
    "print(\"Training data RMSE:\", round(rmse_train.collect()[0][0]))\n",
    "\n",
    "# Test evaluation RMSE\n",
    "rmse_test = test_snowpark_df.select(F.sqrt(F.mean((F.col(\"shift_sales\") \n",
    "                                                   - F.call_udf(\"udf_linreg_predict_location_sales\",\n",
    "                                                                [F.col(c) for c in feature_cols]))**2)))\n",
    "\n",
    "print(\"Test data RMSE:\", round(rmse_test.collect()[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90d7a296",
   "metadata": {},
   "source": [
    "![model_utilization.png](<../../images/tasty_bytes/model_utilization.png>)\n",
    "Now that our model is built and deployed, let's see it in action! We will find the best place to park in Vancouver for tomorrow morning's shift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d702b52",
   "metadata": {},
   "source": [
    "![problem_overview.png](<../../images/tasty_bytes/problem_overview.png>)\n",
    "## Predict Location Sales for the Next Shift\n",
    "We will filter to the morning shift of the first future date in our data and Vancouver locations. We then call our inference user-defined function to get predicted shift sales at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87855df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date to predict\n",
    "date_tomorrow = snowpark_df.filter(F.col(\"shift_sales\").is_null()).select(F.min(\"date\")).collect()[0][0]\n",
    "# Filter to tomorrow's date and the morning shift in Vancouver\n",
    "location_predictions_df = snowpark_df.filter((F.col(\"date\") == date_tomorrow) \n",
    "                                             & (F.col(\"shift\") == 1) \n",
    "                                             & (F.col(\"city\")==\"Vancouver\"))\n",
    "\n",
    "# Get predictions\n",
    "location_predictions_df = location_predictions_df.select(\n",
    "    \"location_id\", \n",
    "    \"latitude\", \n",
    "    \"longitude\",\n",
    "    F.call_udf(\"udf_linreg_predict_location_sales\", [F.col(c) for c in feature_cols]).alias(\"predicted_shift_sales\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3251f453",
   "metadata": {},
   "source": [
    "## Visualize on a Map\n",
    "The yellow dots indicate higher predicted sales locations and the purple dots indicate lower predicted sales. We will use this insight to ensure that our drivers are parking at the high-value locations. <br>\n",
    "**Value:** Updated predictions readily available to drive towards our corporate goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull location predictions into a pandas DataFrame\n",
    "predictions_df = location_predictions_df.to_pandas()\n",
    "\n",
    "# Visualize on a map\n",
    "pio.renderers.default='notebook'\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    predictions_df, \n",
    "    lat=\"LATITUDE\", \n",
    "    lon=\"LONGITUDE\", \n",
    "    hover_name=\"LOCATION_ID\", \n",
    "    size=\"PREDICTED_SHIFT_SALES\",\n",
    "    color=\"PREDICTED_SHIFT_SALES\",\n",
    "    zoom=8, \n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6fa95b4",
   "metadata": {},
   "source": [
    "![blue_frostbyte_primary_logo.png](<../../images/tasty_bytes/blue_frostbyte_primary_logo.png>)\n",
    "**Quickstart: \t\tTasty Bytes - Snowpark 101 for Data Science**<br />\n",
    "Script: \ttasty_bytes_snowpark_101.ipynb <br />   \t\n",
    "Create Date:    2023-05-19 <br />\n",
    "Author:         Marie Coolsaet <br />\n",
    "\n",
    "**Description:**<br />\n",
    "A linear regression model is trained and deployed in Snowflake to predict the best location for food trucks to park during their shifts.\n",
    "\n",
    "\n",
    "**Summary of Changes:**<br />\n",
    "\n",
    "| Date(yyyy-mm-dd) | Author         | <div style=\"width:290px\">Comments</div>                        |\n",
    "| :---             | :---           | :---                                                  |\n",
    "|2023-05-19         |Marie Coolsaet | Initial Quickstart Release|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "87393b85a9e84f5acf85b4897642fb1f420cfcaf148d9618810d560b47e94533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
